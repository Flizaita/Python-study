{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43 34 64 59\n",
      "0.535 0.558441558442 0.421568627451 0.480446927374\n",
      "[0.71918767507002801, 0.70868347338935567, 0.63515406162464982, 0.69192677070828335]\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.metrics as skm\n",
    "\n",
    "data = pd.read_csv(\"classification.csv\")\n",
    "\n",
    "tp = len(data[(data['true'] == 1) & (data['pred'] ==1)])\n",
    "fp = len(data[(data['true'] == 0) & (data['pred'] ==1)])\n",
    "tn = len(data[(data['true'] == 0) & (data['pred'] ==0)])\n",
    "fn = len(data[(data['true'] == 1) & (data['pred'] ==0)])\n",
    "\n",
    "print tp, fp, tn, fn\n",
    "\n",
    "acc = skm.accuracy_score(data['true'], data['pred'])\n",
    "precision = skm.precision_score(data['true'], data['pred'])\n",
    "recall = skm.recall_score(data['true'], data['pred'])\n",
    "f_score = skm.f1_score(data['true'], data['pred'])\n",
    "\n",
    "print acc, precision, recall, f_score\n",
    "\n",
    "classif_num = len(class_scores.columns)\n",
    "class_scores = pd.read_csv(\"scores.csv\") \n",
    "auc_scores = [skm.roc_auc_score(class_scores['true'], class_scores.ix[:,i]) for i in xrange(1,classif_num )]\n",
    "\n",
    "print auc_scores\n",
    "\n",
    "presicion_l = list(xrange(classif_num - 1))\n",
    "recall_l = list(xrange(classif_num - 1))\n",
    "\n",
    "for i in xrange(1,classif_num):\n",
    "    presicion_l[i-1], recall_l[i-1],_ = skm.precision_recall_curve(class_scores['true'], class_scores.ix[:,i])\n",
    "    \n",
    "maximas = []\n",
    "\n",
    "for i in xrange(classif_num - 1):\n",
    "    dataf = pd.DataFrame({'P': presicion_l[i], 'R': recall_l[i]})\n",
    "    temp = dataf[dataf['R'] >= 0.7]\n",
    "    maximas.append(max(temp['P']))\n",
    "    \n",
    "print np.argmax(maximas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     FullDescription LocationNormalized  \\\n",
      "0  international sales manager london     k      ...             London   \n",
      "1  an ideal opportunity for an individual that ha...             London   \n",
      "2  online content and brand manager   luxury reta...  South East London   \n",
      "3  a great local marketleader is seeking a perman...            Dereham   \n",
      "4  registered nurse   rgn  nursing home for young...   Sutton Coldfield   \n",
      "\n",
      "  ContractTime  SalaryNormalized  \n",
      "0    permanent             33000  \n",
      "1    permanent             50000  \n",
      "2    permanent             40000  \n",
      "3    permanent             22500  \n",
      "4          NaN             20355  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"salary-train.csv\")\n",
    "data_test = pd.read_csv(\"salary-test-mini.csv\")\n",
    "\n",
    "data['FullDescription'] = [text.lower() for text in data['FullDescription']]\n",
    "data['FullDescription'] = data['FullDescription'].replace('[^a-z0-9]', ' ', regex = True)\n",
    "\n",
    "data_test['FullDescription'] = [text.lower() for text in data_test['FullDescription']]\n",
    "data_test['FullDescription'] = data_test['FullDescription'].replace('[^a-z0-9]', ' ', regex = True)\n",
    "\n",
    "print data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     FullDescription LocationNormalized  \\\n",
      "0  international sales manager london     k      ...             London   \n",
      "1  an ideal opportunity for an individual that ha...             London   \n",
      "2  online content and brand manager   luxury reta...  South East London   \n",
      "3  a great local marketleader is seeking a perman...            Dereham   \n",
      "4  registered nurse   rgn  nursing home for young...   Sutton Coldfield   \n",
      "\n",
      "  ContractTime  SalaryNormalized  \n",
      "0    permanent             33000  \n",
      "1    permanent             50000  \n",
      "2    permanent             40000  \n",
      "3    permanent             22500  \n",
      "4          NaN             20355  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(min_df = 5)\n",
    "X_text = vectorizer.fit_transform(data['FullDescription'])\n",
    "X_test = vectorizer.transform(data_test['FullDescription'])\n",
    "print data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data['LocationNormalized'].fillna('nan', inplace=True)\n",
    "data['ContractTime'].fillna('nan', inplace=True)\n",
    "\n",
    "data_test['LocationNormalized'].fillna('nan', inplace=True)\n",
    "data_test['ContractTime'].fillna('nan', inplace=True)\n",
    "\n",
    "print data.head()\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "enc = DictVectorizer()\n",
    "X_train_categ = enc.fit_transform(data[['LocationNormalized', 'ContractTime']].to_dict('records'))\n",
    "X_test_categ = enc.transform(data_test[['LocationNormalized', 'ContractTime']].to_dict('records'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import vstack, hstack,coo_matrix\n",
    "\n",
    "X_train_f = hstack([X_text, X_train_categ])\n",
    "X_test_f = hstack([X_test, X_test_categ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ridge(alpha=1, copy_X=True, fit_intercept=True, max_iter=None,\n",
       "   normalize=False, random_state=241, solver='auto', tol=0.001)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "clf = Ridge(alpha = 1, random_state = 241)\n",
    "clf.fit(X_train_f, data['SalaryNormalized'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 56555.61500155  37188.32442618]\n"
     ]
    }
   ],
   "source": [
    "print clf.predict(X_test_f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method of main components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         date          ^DJI\n",
      "0  2013-09-23  15401.379883\n",
      "1  2013-09-24  15334.589844\n",
      "2  2013-09-25  15273.259766\n",
      "3  2013-09-26  15328.299805\n",
      "4  2013-09-27  15258.240234\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "prices = pd.read_csv(\"close_prices.csv\")\n",
    "index = pd.read_csv(\"djia_index.csv\")\n",
    "\n",
    "prices = prices.drop('date',1)\n",
    "\n",
    "print index.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.73897118  0.11007169  0.04995088  0.0287492   0.02215448  0.01931577\n",
      "  0.00674853  0.00614091  0.00320594  0.00305611]\n",
      "[[ 1.          0.90965222]\n",
      " [ 0.90965222  1.        ]]\n",
      "V\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "pca = PCA(n_components = 10)\n",
    "pca.fit(prices)\n",
    "\n",
    "print pca.explained_variance_ratio_\n",
    "\n",
    "tr_prices = pd.DataFrame(pca.transform(prices))\n",
    "\n",
    "print np.corrcoef(tr_prices[0], index['^DJI'])\n",
    "\n",
    "print prices.columns.values[np.argmax(pca.components_[0])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Sex  Length  Diameter  Height  WholeWeight  ShuckedWeight  \\\n",
      "0       1   0.455     0.365   0.095       0.5140         0.2245   \n",
      "1       1   0.350     0.265   0.090       0.2255         0.0995   \n",
      "2      -1   0.530     0.420   0.135       0.6770         0.2565   \n",
      "3       1   0.440     0.365   0.125       0.5160         0.2155   \n",
      "4       0   0.330     0.255   0.080       0.2050         0.0895   \n",
      "5       0   0.425     0.300   0.095       0.3515         0.1410   \n",
      "6      -1   0.530     0.415   0.150       0.7775         0.2370   \n",
      "7      -1   0.545     0.425   0.125       0.7680         0.2940   \n",
      "8       1   0.475     0.370   0.125       0.5095         0.2165   \n",
      "9      -1   0.550     0.440   0.150       0.8945         0.3145   \n",
      "10     -1   0.525     0.380   0.140       0.6065         0.1940   \n",
      "11      1   0.430     0.350   0.110       0.4060         0.1675   \n",
      "12      1   0.490     0.380   0.135       0.5415         0.2175   \n",
      "13     -1   0.535     0.405   0.145       0.6845         0.2725   \n",
      "14     -1   0.470     0.355   0.100       0.4755         0.1675   \n",
      "15      1   0.500     0.400   0.130       0.6645         0.2580   \n",
      "16      0   0.355     0.280   0.085       0.2905         0.0950   \n",
      "17     -1   0.440     0.340   0.100       0.4510         0.1880   \n",
      "18      1   0.365     0.295   0.080       0.2555         0.0970   \n",
      "19      1   0.450     0.320   0.100       0.3810         0.1705   \n",
      "20      1   0.355     0.280   0.095       0.2455         0.0955   \n",
      "21      0   0.380     0.275   0.100       0.2255         0.0800   \n",
      "22     -1   0.565     0.440   0.155       0.9395         0.4275   \n",
      "23     -1   0.550     0.415   0.135       0.7635         0.3180   \n",
      "24     -1   0.615     0.480   0.165       1.1615         0.5130   \n",
      "25     -1   0.560     0.440   0.140       0.9285         0.3825   \n",
      "26     -1   0.580     0.450   0.185       0.9955         0.3945   \n",
      "27      1   0.590     0.445   0.140       0.9310         0.3560   \n",
      "28      1   0.605     0.475   0.180       0.9365         0.3940   \n",
      "29      1   0.575     0.425   0.140       0.8635         0.3930   \n",
      "...   ...     ...       ...     ...          ...            ...   \n",
      "4147    1   0.695     0.550   0.195       1.6645         0.7270   \n",
      "4148    1   0.770     0.605   0.175       2.0505         0.8005   \n",
      "4149    0   0.280     0.215   0.070       0.1240         0.0630   \n",
      "4150    0   0.330     0.230   0.080       0.1400         0.0565   \n",
      "4151    0   0.350     0.250   0.075       0.1695         0.0835   \n",
      "4152    0   0.370     0.280   0.090       0.2180         0.0995   \n",
      "4153    0   0.430     0.315   0.115       0.3840         0.1885   \n",
      "4154    0   0.435     0.330   0.095       0.3930         0.2190   \n",
      "4155    0   0.440     0.350   0.110       0.3805         0.1575   \n",
      "4156    1   0.475     0.370   0.110       0.4895         0.2185   \n",
      "4157    1   0.475     0.360   0.140       0.5135         0.2410   \n",
      "4158    0   0.480     0.355   0.110       0.4495         0.2010   \n",
      "4159   -1   0.560     0.440   0.135       0.8025         0.3500   \n",
      "4160   -1   0.585     0.475   0.165       1.0530         0.4580   \n",
      "4161   -1   0.585     0.455   0.170       0.9945         0.4255   \n",
      "4162    1   0.385     0.255   0.100       0.3175         0.1370   \n",
      "4163    0   0.390     0.310   0.085       0.3440         0.1810   \n",
      "4164    0   0.390     0.290   0.100       0.2845         0.1255   \n",
      "4165    0   0.405     0.300   0.085       0.3035         0.1500   \n",
      "4166    0   0.475     0.365   0.115       0.4990         0.2320   \n",
      "4167    1   0.500     0.380   0.125       0.5770         0.2690   \n",
      "4168   -1   0.515     0.400   0.125       0.6150         0.2865   \n",
      "4169    1   0.520     0.385   0.165       0.7910         0.3750   \n",
      "4170    1   0.550     0.430   0.130       0.8395         0.3155   \n",
      "4171    1   0.560     0.430   0.155       0.8675         0.4000   \n",
      "4172   -1   0.565     0.450   0.165       0.8870         0.3700   \n",
      "4173    1   0.590     0.440   0.135       0.9660         0.4390   \n",
      "4174    1   0.600     0.475   0.205       1.1760         0.5255   \n",
      "4175   -1   0.625     0.485   0.150       1.0945         0.5310   \n",
      "4176    1   0.710     0.555   0.195       1.9485         0.9455   \n",
      "\n",
      "      VisceraWeight  ShellWeight  Rings  \n",
      "0            0.1010       0.1500     15  \n",
      "1            0.0485       0.0700      7  \n",
      "2            0.1415       0.2100      9  \n",
      "3            0.1140       0.1550     10  \n",
      "4            0.0395       0.0550      7  \n",
      "5            0.0775       0.1200      8  \n",
      "6            0.1415       0.3300     20  \n",
      "7            0.1495       0.2600     16  \n",
      "8            0.1125       0.1650      9  \n",
      "9            0.1510       0.3200     19  \n",
      "10           0.1475       0.2100     14  \n",
      "11           0.0810       0.1350     10  \n",
      "12           0.0950       0.1900     11  \n",
      "13           0.1710       0.2050     10  \n",
      "14           0.0805       0.1850     10  \n",
      "15           0.1330       0.2400     12  \n",
      "16           0.0395       0.1150      7  \n",
      "17           0.0870       0.1300     10  \n",
      "18           0.0430       0.1000      7  \n",
      "19           0.0750       0.1150      9  \n",
      "20           0.0620       0.0750     11  \n",
      "21           0.0490       0.0850     10  \n",
      "22           0.2140       0.2700     12  \n",
      "23           0.2100       0.2000      9  \n",
      "24           0.3010       0.3050     10  \n",
      "25           0.1880       0.3000     11  \n",
      "26           0.2720       0.2850     11  \n",
      "27           0.2340       0.2800     12  \n",
      "28           0.2190       0.2950     15  \n",
      "29           0.2270       0.2000     11  \n",
      "...             ...          ...    ...  \n",
      "4147         0.3600       0.4450     11  \n",
      "4148         0.5260       0.3550     11  \n",
      "4149         0.0215       0.0300      6  \n",
      "4150         0.0365       0.0460      7  \n",
      "4151         0.0355       0.0410      6  \n",
      "4152         0.0545       0.0615      7  \n",
      "4153         0.0715       0.1100      8  \n",
      "4154         0.0750       0.0885      6  \n",
      "4155         0.0895       0.1150      6  \n",
      "4156         0.1070       0.1460      8  \n",
      "4157         0.1045       0.1550      8  \n",
      "4158         0.0890       0.1400      8  \n",
      "4159         0.1615       0.2590      9  \n",
      "4160         0.2170       0.3000     11  \n",
      "4161         0.2630       0.2845     11  \n",
      "4162         0.0680       0.0920      8  \n",
      "4163         0.0695       0.0790      7  \n",
      "4164         0.0635       0.0810      7  \n",
      "4165         0.0505       0.0880      7  \n",
      "4166         0.0885       0.1560     10  \n",
      "4167         0.1265       0.1535      9  \n",
      "4168         0.1230       0.1765      8  \n",
      "4169         0.1800       0.1815     10  \n",
      "4170         0.1955       0.2405     10  \n",
      "4171         0.1720       0.2290      8  \n",
      "4172         0.2390       0.2490     11  \n",
      "4173         0.2145       0.2605     10  \n",
      "4174         0.2875       0.3080      9  \n",
      "4175         0.2610       0.2960     10  \n",
      "4176         0.3765       0.4950     12  \n",
      "\n",
      "[4177 rows x 9 columns]\n",
      "      Sex  Length  Diameter  Height  WholeWeight  ShuckedWeight  \\\n",
      "0       1   0.455     0.365   0.095       0.5140         0.2245   \n",
      "1       1   0.350     0.265   0.090       0.2255         0.0995   \n",
      "2      -1   0.530     0.420   0.135       0.6770         0.2565   \n",
      "3       1   0.440     0.365   0.125       0.5160         0.2155   \n",
      "4       0   0.330     0.255   0.080       0.2050         0.0895   \n",
      "5       0   0.425     0.300   0.095       0.3515         0.1410   \n",
      "6      -1   0.530     0.415   0.150       0.7775         0.2370   \n",
      "7      -1   0.545     0.425   0.125       0.7680         0.2940   \n",
      "8       1   0.475     0.370   0.125       0.5095         0.2165   \n",
      "9      -1   0.550     0.440   0.150       0.8945         0.3145   \n",
      "10     -1   0.525     0.380   0.140       0.6065         0.1940   \n",
      "11      1   0.430     0.350   0.110       0.4060         0.1675   \n",
      "12      1   0.490     0.380   0.135       0.5415         0.2175   \n",
      "13     -1   0.535     0.405   0.145       0.6845         0.2725   \n",
      "14     -1   0.470     0.355   0.100       0.4755         0.1675   \n",
      "15      1   0.500     0.400   0.130       0.6645         0.2580   \n",
      "16      0   0.355     0.280   0.085       0.2905         0.0950   \n",
      "17     -1   0.440     0.340   0.100       0.4510         0.1880   \n",
      "18      1   0.365     0.295   0.080       0.2555         0.0970   \n",
      "19      1   0.450     0.320   0.100       0.3810         0.1705   \n",
      "20      1   0.355     0.280   0.095       0.2455         0.0955   \n",
      "21      0   0.380     0.275   0.100       0.2255         0.0800   \n",
      "22     -1   0.565     0.440   0.155       0.9395         0.4275   \n",
      "23     -1   0.550     0.415   0.135       0.7635         0.3180   \n",
      "24     -1   0.615     0.480   0.165       1.1615         0.5130   \n",
      "25     -1   0.560     0.440   0.140       0.9285         0.3825   \n",
      "26     -1   0.580     0.450   0.185       0.9955         0.3945   \n",
      "27      1   0.590     0.445   0.140       0.9310         0.3560   \n",
      "28      1   0.605     0.475   0.180       0.9365         0.3940   \n",
      "29      1   0.575     0.425   0.140       0.8635         0.3930   \n",
      "...   ...     ...       ...     ...          ...            ...   \n",
      "4147    1   0.695     0.550   0.195       1.6645         0.7270   \n",
      "4148    1   0.770     0.605   0.175       2.0505         0.8005   \n",
      "4149    0   0.280     0.215   0.070       0.1240         0.0630   \n",
      "4150    0   0.330     0.230   0.080       0.1400         0.0565   \n",
      "4151    0   0.350     0.250   0.075       0.1695         0.0835   \n",
      "4152    0   0.370     0.280   0.090       0.2180         0.0995   \n",
      "4153    0   0.430     0.315   0.115       0.3840         0.1885   \n",
      "4154    0   0.435     0.330   0.095       0.3930         0.2190   \n",
      "4155    0   0.440     0.350   0.110       0.3805         0.1575   \n",
      "4156    1   0.475     0.370   0.110       0.4895         0.2185   \n",
      "4157    1   0.475     0.360   0.140       0.5135         0.2410   \n",
      "4158    0   0.480     0.355   0.110       0.4495         0.2010   \n",
      "4159   -1   0.560     0.440   0.135       0.8025         0.3500   \n",
      "4160   -1   0.585     0.475   0.165       1.0530         0.4580   \n",
      "4161   -1   0.585     0.455   0.170       0.9945         0.4255   \n",
      "4162    1   0.385     0.255   0.100       0.3175         0.1370   \n",
      "4163    0   0.390     0.310   0.085       0.3440         0.1810   \n",
      "4164    0   0.390     0.290   0.100       0.2845         0.1255   \n",
      "4165    0   0.405     0.300   0.085       0.3035         0.1500   \n",
      "4166    0   0.475     0.365   0.115       0.4990         0.2320   \n",
      "4167    1   0.500     0.380   0.125       0.5770         0.2690   \n",
      "4168   -1   0.515     0.400   0.125       0.6150         0.2865   \n",
      "4169    1   0.520     0.385   0.165       0.7910         0.3750   \n",
      "4170    1   0.550     0.430   0.130       0.8395         0.3155   \n",
      "4171    1   0.560     0.430   0.155       0.8675         0.4000   \n",
      "4172   -1   0.565     0.450   0.165       0.8870         0.3700   \n",
      "4173    1   0.590     0.440   0.135       0.9660         0.4390   \n",
      "4174    1   0.600     0.475   0.205       1.1760         0.5255   \n",
      "4175   -1   0.625     0.485   0.150       1.0945         0.5310   \n",
      "4176    1   0.710     0.555   0.195       1.9485         0.9455   \n",
      "\n",
      "      VisceraWeight  ShellWeight  \n",
      "0            0.1010       0.1500  \n",
      "1            0.0485       0.0700  \n",
      "2            0.1415       0.2100  \n",
      "3            0.1140       0.1550  \n",
      "4            0.0395       0.0550  \n",
      "5            0.0775       0.1200  \n",
      "6            0.1415       0.3300  \n",
      "7            0.1495       0.2600  \n",
      "8            0.1125       0.1650  \n",
      "9            0.1510       0.3200  \n",
      "10           0.1475       0.2100  \n",
      "11           0.0810       0.1350  \n",
      "12           0.0950       0.1900  \n",
      "13           0.1710       0.2050  \n",
      "14           0.0805       0.1850  \n",
      "15           0.1330       0.2400  \n",
      "16           0.0395       0.1150  \n",
      "17           0.0870       0.1300  \n",
      "18           0.0430       0.1000  \n",
      "19           0.0750       0.1150  \n",
      "20           0.0620       0.0750  \n",
      "21           0.0490       0.0850  \n",
      "22           0.2140       0.2700  \n",
      "23           0.2100       0.2000  \n",
      "24           0.3010       0.3050  \n",
      "25           0.1880       0.3000  \n",
      "26           0.2720       0.2850  \n",
      "27           0.2340       0.2800  \n",
      "28           0.2190       0.2950  \n",
      "29           0.2270       0.2000  \n",
      "...             ...          ...  \n",
      "4147         0.3600       0.4450  \n",
      "4148         0.5260       0.3550  \n",
      "4149         0.0215       0.0300  \n",
      "4150         0.0365       0.0460  \n",
      "4151         0.0355       0.0410  \n",
      "4152         0.0545       0.0615  \n",
      "4153         0.0715       0.1100  \n",
      "4154         0.0750       0.0885  \n",
      "4155         0.0895       0.1150  \n",
      "4156         0.1070       0.1460  \n",
      "4157         0.1045       0.1550  \n",
      "4158         0.0890       0.1400  \n",
      "4159         0.1615       0.2590  \n",
      "4160         0.2170       0.3000  \n",
      "4161         0.2630       0.2845  \n",
      "4162         0.0680       0.0920  \n",
      "4163         0.0695       0.0790  \n",
      "4164         0.0635       0.0810  \n",
      "4165         0.0505       0.0880  \n",
      "4166         0.0885       0.1560  \n",
      "4167         0.1265       0.1535  \n",
      "4168         0.1230       0.1765  \n",
      "4169         0.1800       0.1815  \n",
      "4170         0.1955       0.2405  \n",
      "4171         0.1720       0.2290  \n",
      "4172         0.2390       0.2490  \n",
      "4173         0.2145       0.2605  \n",
      "4174         0.2875       0.3080  \n",
      "4175         0.2610       0.2960  \n",
      "4176         0.3765       0.4950  \n",
      "\n",
      "[4177 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"abalone.csv\")\n",
    "data['Sex'] = data['Sex'].map(lambda x: 1 if x == 'M' else (-1 if x == 'F' else 0))\n",
    "\n",
    "Y = data['Rings']\n",
    "X = data.drop('Rings',1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.102163125849\n",
      "{'n_estimators': 1}\n",
      "0.338412959313\n",
      "{'n_estimators': 2}\n",
      "0.403584837629\n",
      "{'n_estimators': 3}\n",
      "0.442722234326\n",
      "{'n_estimators': 4}\n",
      "0.464020401671\n",
      "{'n_estimators': 5}\n",
      "0.470580138647\n",
      "{'n_estimators': 6}\n",
      "0.47582658808\n",
      "{'n_estimators': 7}\n",
      "0.481738243326\n",
      "{'n_estimators': 8}\n",
      "0.488342750783\n",
      "{'n_estimators': 9}\n",
      "0.494458152768\n",
      "{'n_estimators': 10}\n",
      "0.493391074362\n",
      "{'n_estimators': 11}\n",
      "0.497961126662\n",
      "{'n_estimators': 12}\n",
      "0.50213210699\n",
      "{'n_estimators': 13}\n",
      "0.506424775497\n",
      "{'n_estimators': 14}\n",
      "0.508328085298\n",
      "{'n_estimators': 15}\n",
      "0.510509452008\n",
      "{'n_estimators': 16}\n",
      "0.513845502985\n",
      "{'n_estimators': 17}\n",
      "0.516324176384\n",
      "{'n_estimators': 18}\n",
      "0.519031046713\n",
      "{'n_estimators': 19}\n",
      "0.518670729546\n",
      "{'n_estimators': 20}\n",
      "0.51983247513\n",
      "{'n_estimators': 21}\n",
      "0.520155890139\n",
      "{'n_estimators': 22}\n",
      "0.521015079498\n",
      "{'n_estimators': 23}\n",
      "0.522401328334\n",
      "{'n_estimators': 24}\n",
      "0.522615326396\n",
      "{'n_estimators': 25}\n",
      "0.523804761492\n",
      "{'n_estimators': 26}\n",
      "0.524120059055\n",
      "{'n_estimators': 27}\n",
      "0.525050806268\n",
      "{'n_estimators': 28}\n",
      "0.52596755301\n",
      "{'n_estimators': 29}\n",
      "0.526534754321\n",
      "{'n_estimators': 30}\n",
      "0.527096390596\n",
      "{'n_estimators': 31}\n",
      "0.52838584687\n",
      "{'n_estimators': 32}\n",
      "0.529612686174\n",
      "{'n_estimators': 33}\n",
      "0.529559798833\n",
      "{'n_estimators': 34}\n",
      "0.529513146047\n",
      "{'n_estimators': 35}\n",
      "0.529595647382\n",
      "{'n_estimators': 36}\n",
      "0.529127350278\n",
      "{'n_estimators': 37}\n",
      "0.529156227253\n",
      "{'n_estimators': 38}\n",
      "0.529236970941\n",
      "{'n_estimators': 39}\n",
      "0.529062998534\n",
      "{'n_estimators': 40}\n",
      "0.529338105916\n",
      "{'n_estimators': 41}\n",
      "0.529569243519\n",
      "{'n_estimators': 42}\n",
      "0.529371420704\n",
      "{'n_estimators': 43}\n",
      "0.529115268453\n",
      "{'n_estimators': 44}\n",
      "0.52837143582\n",
      "{'n_estimators': 45}\n",
      "0.528516251032\n",
      "{'n_estimators': 46}\n",
      "0.528700173877\n",
      "{'n_estimators': 47}\n",
      "0.529607881742\n",
      "{'n_estimators': 48}\n",
      "0.530389749611\n",
      "{'n_estimators': 49}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "import numpy as np\n",
    "\n",
    "grid = {\"n_estimators\": np.arange(1,50)}\n",
    "ens = RandomForestRegressor(random_state = 1)\n",
    "cv = KFold(Y.size, shuffle=True, n_folds=5, random_state = 1)\n",
    "gs = GridSearchCV(ens, grid, scoring = 'r2', cv = cv)\n",
    "gs.fit(X,Y)\n",
    "for a in gs.grid_scores_:\n",
    "    print(a.mean_validation_score)\n",
    "    print(a.parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"gbm-data.csv\")\n",
    "\n",
    "data_y = data['Activity']\n",
    "data_X = data.drop('Activity', 1)\n",
    "\n",
    "y = data_y.values\n",
    "X = data_X.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.2613            2.08m\n",
      "         2           1.1715            2.14m\n",
      "         3           1.1009            2.20m\n",
      "         4           1.0529            2.22m\n",
      "         5           1.0130            2.18m\n",
      "         6           0.9740            2.13m\n",
      "         7           0.9475            2.06m\n",
      "         8           0.9197            2.02m\n",
      "         9           0.8979            1.98m\n",
      "        10           0.8730            1.95m\n",
      "        20           0.7207            2.01m\n",
      "        30           0.6055            1.92m\n",
      "        40           0.5244            1.84m\n",
      "        50           0.4501            1.79m\n",
      "        60           0.3908            1.72m\n",
      "        70           0.3372            1.63m\n",
      "        80           0.3009            1.59m\n",
      "        90           0.2603            1.52m\n",
      "       100           0.2327            1.44m\n",
      "       200           0.0835           29.80s\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEACAYAAAC9Gb03AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x21155f28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36 0.530439819735\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import log_loss\n",
    "import numpy as np\n",
    "\n",
    "def print_plot(train_loss, test_loss ):\n",
    "    import matplotlib.pyplot as plt\n",
    "    %matplotlib inline\n",
    "    plt.figure()\n",
    "    plt.plot(test_loss, 'r', linewidth=2)\n",
    "    plt.plot(train_loss, 'g', linewidth=2)\n",
    "    plt.legend(['test', 'train'])\n",
    "    plt.show()\n",
    "\n",
    "def sigm(y):\n",
    "    return 1/(1 + np.exp(-1 * y))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.8, random_state = 241)\n",
    "\n",
    "\n",
    "for el in [0.2]:\n",
    "    clf = GradientBoostingClassifier(n_estimators = 250, learning_rate = el, verbose = True, random_state = 241)\n",
    "    clf.fit(X_train, y_train)\n",
    "    test_loss = np.empty(len(clf.estimators_))\n",
    "    train_loss = np.empty(len(clf.estimators_))\n",
    "    for i, pred in enumerate(clf.staged_decision_function(X_train)):\n",
    "        train_loss[i] = log_loss(y_train, sigm(pred))\n",
    "    for i, pred in enumerate(clf.staged_decision_function(X_test)):\n",
    "        test_loss[i] = log_loss(y_test, sigm(pred))\n",
    "    print_plot(train_loss, test_loss)\n",
    "    m = np.argmin(test_loss)\n",
    "    print m, test_loss[m]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.54138128618\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "ens = RandomForestClassifier(n_estimators = 36, random_state = 241)\n",
    "ens.fit(X_train, y_train)\n",
    "print log_loss(y_test, ens.predict_proba(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
